{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69702,"status":"ok","timestamp":1705594074527,"user":{"displayName":"Samaneh Shahpuori","userId":"07452399669550385873"},"user_tz":-60},"id":"EHBrChiqSMyw","outputId":"9902a3f9-cfa8-4704-fec7-32bcdc3fed4a"},"outputs":[],"source":["\n","from monai.transforms import (Compose)\n","from monai.inferers import sliding_window_inference\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from monai.transforms import Compose, Invertd, SaveImaged\n","from monai.inferers import sliding_window_inference\n","from monai.data import decollate_batch\n","import torch\n","import json\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25866,"status":"ok","timestamp":1705594100378,"user":{"displayName":"Samaneh Shahpuori","userId":"07452399669550385873"},"user_tz":-60},"id":"Xa93Ahdf17k7","outputId":"9fab217d-4e03-4cc6-f83e-165e855c561e"},"outputs":[],"source":["config_file = 'config.json'\n","\n","with open(config_file, 'r') as f:\n","    config = json.load(f)\n","\n","ga_data_dir = config[\"ga_data_dir\"]\n","fdg_data_dir = config[\"fdg_data_dir\"]\n","log_dir = config[\"log_dir\"]\n","output_dir = config[\"output_dir\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from data_preparation import ExtrenalRadioSetSetHandling\n","\n","# data_handler = ExtrenalRadioSetSetHandling(fdg_data_dir, train_mode=\"NAC\", target_mode=\"MAC\")\n","\n","# test_files = data_handler.get_data()\n","# len(test_files)\n","# from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, RandSpatialCropSamplesd, CenterSpatialCropd\n","# from monai.data import CacheDataset, DataLoader, Dataset\n","\n","\n","# spacing = (4.07, 4.07, 3.00)\n","# spatial_size = (168, 168, 640)\n","\n","# test_transforms = Compose([\n","#         LoadImaged(keys=[\"image\", \"target\"]),\n","#         EnsureChannelFirstd(keys=[\"image\", \"target\"]),\n","#         Spacingd(keys=[\"image\", \"target\"], pixdim=spacing, mode= 'trilinear'),\n","#         SpatialPadd(keys=[\"image\", \"target\"], spatial_size=spatial_size, mode='constant'),  # Ensure minimum size\n","#         CenterSpatialCropd(keys=[\"image\", \"target\"], roi_size=spatial_size),  # Ensure uniform size\n","#     ])\n","\n","# test_ds = CacheDataset(data=test_files, transform=test_transforms, cache_rate=1.0, num_workers=1)\n","# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data_preparation import DataHandling \n","\n","data_handler = DataHandling(ga_data_dir, train_mode=\"NAC\", target_mode=\"MAC\")\n","\n","train_files = data_handler.get_data_split('train')\n","val_files = data_handler.get_data_split('val')\n","test_files = data_handler.get_data_split('test')\n","print(len(train_files))\n","print(len(val_files))\n","print(len(test_files))\n","\n","from data_preparation import LoaderFactory\n","loader_factory = LoaderFactory(\n","    train_files=train_files,\n","    val_files=val_files,\n","    test_files=test_files,\n","    patch_size = [168, 168, 16],\n","    spacing = [4.07, 4.07, 3.00],\n","    # spacing = [1.92, 1.92, 3.27],\n","    spatial_size = (168, 168, 320)\n","    # spatial_size = (168, 168, 640)\n","    # spatial_size = (336, 336, 640)\n","    )\n","\n","# Get the DataLoader for each dataset type\n","# train_loader = loader_factory.get_loader('train', batch_size=4, num_workers=2, shuffle=True)\n","# val_loader = loader_factory.get_loader('val', batch_size=1, num_workers=2, shuffle=False)\n","test_loader = loader_factory.get_loader('test', batch_size=1, num_workers=2, shuffle=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from monai.transforms import LoadImage\n","\n","# # Assuming 'test_files' contains your image paths like {'image': 'path/to/image.nii', 'target': 'path/to/target.nii'}\n","# loader = LoadImage(image_only=True)\n","\n","# for i in test_files:\n","#     image_path = i['image']  # Replace 'test_files[0]['image']' with the actual path if necessary\n","#     original_image = loader(image_path)\n","#     print(f\"Original image size: {original_image.shape}\")\n","\n","# print('--------------------------')\n","# for batch in test_loader:\n","#     transformed_image = batch['image']\n","#     print(f\"Transformed image size: {transformed_image.shape}\")\n","#       # Break after the first batch since you're only interested in one image\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data_preparation import visualize_axial_slice, visualize_axial_slice2\n","\n","\n","slice_index = 100  # Example slice index.\n","visualize_axial_slice2(test_loader, slice_index)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["slice_index = 200  # Example slice index.\n","visualize_axial_slice(test_loader, slice_index)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import find_last_saved_model, parse_loss_values\n","\n","# log_filename = 'log_2_26_8_17.txt'\n","# log_filename = 'log_3_4_12_4.txt'\n","# log_filename = 'log_3_4_12_41.txt'\n","# log_filename = 'log_3_18.txt'\n","# log_filename = 'log_3_27_8_47.txt'\n","# log_filename = 'log_3_28_10_41.txt'\n","log_filename = 'log_3_28_20_48.txt'\n","\n","log_filepath = log_dir + '/'+ log_filename\n","bestmodel_filename, best_metric, best_epoch = find_last_saved_model(log_filepath)\n","print(f\"Model Filename: {bestmodel_filename}, Best Metric: {best_metric}, Epoch: {best_epoch}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bestmodel_filename =  'model_3_29_0_30.pth'\n","# best_metric = 0.4701\n","# best_epoch = 34"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_losses, val_losses = parse_loss_values(log_filepath)\n","\n","max_epochs = len(train_losses)\n","val_interval = 2  # Update this if your validation interval is different\n","\n","# Plotting\n","plt.figure(figsize=(14, 6))\n","plt.plot(range(1, max_epochs + 1), train_losses, label='Training Loss', color='blue', alpha=0.9)\n","plt.plot(range(2, max_epochs + 1, val_interval), val_losses, label='Validation Loss', color='orange', alpha=0.8)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Losses')\n","plt.legend()\n","plt.xticks(np.arange(1, max_epochs + 1, 10))  # Adjust the x-axis ticks if needed\n","plt.show()\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torchviz'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynUNet\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define the device to run the model on\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"]}],"source":["import torch\n","import torch.nn as nn\n","from torchviz import make_dot\n","\n","\n","\n","# Create a dummy input tensor\n","dummy_input = torch.randn(1, in_channels, *([64]*spatial_dims)).to(device)\n","\n","# Perform a forward pass (mock) to get the output\n","output = model(dummy_input)\n","\n","# Visualize the graph - Note that the output might be a tuple if deep_supervision is True\n","# In such cases, use output[0] instead of output\n","dot = make_dot(output, params=dict(model.named_parameters()))\n","\n","# Save the visualization to a file\n","dot.format = 'png'\n","dot.render('dynunet_architecture')\n","\n","# Print the path to the saved visualization file\n","print('The visualization of DynUNet is saved to \"dynunet_architecture.png\"')\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DyUnet is set:\n","Kernel size:  [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n","Strides:  [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 1]]\n"]}],"source":["import os\n","import torch\n","from model_maker import get_network\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = get_network(patch_size = [168, 168, 16], spacing = [4.07, 4.07, 3.00])\n","model = model.to(device)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torchviz'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_network  \u001b[38;5;66;03m# Assuming models.py is in the same directory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a dummy input tensor based on the patch_size\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"]}],"source":["import torch\n","from torchviz import make_dot\n","\n","\n","# Create a dummy input tensor based on the patch_size\n","dummy_input = torch.randn(1, 1, *patch_size).to(device)\n","\n","# Perform a forward pass to get the output\n","# The deep_supervision flag in get_network is set to True, the output will be a list\n","output = model(dummy_input)[0] if isinstance(model(dummy_input), list) else model(dummy_input)\n","\n","# Visualize the graph\n","dot = make_dot(output, params=dict(model.named_parameters()))\n","\n","# Save the visualization to a file\n","dot.format = 'png'\n","dot.render('dynunet_architecture')\n","\n","# Print the path to the saved visualization file\n","print('The visualization of DynUNet is saved to \"dynunet_architecture.png\"')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","# Assuming root_dir is the directory where your model files are stored\n","model_path = os.path.join(log_dir, bestmodel_filename)\n","if os.path.exists(model_path):\n","    print(f\"Model file {bestmodel_filename} is loading.\")\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","else:\n","    print(f\"Model file {bestmodel_filename} not found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract file names\n","test_name = [(os.path.splitext(os.path.basename(file_info['image']))[0], os.path.splitext(os.path.basename(file_info['target']))[0]) for file_info in test_files]\n","test_name\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","test_name = [(os.path.splitext(os.path.basename(file_info['image']))[0], os.path.splitext(os.path.basename(file_info['target']))[0]) for file_info in train_files]\n","test_name\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from data_preparation import visualize_coronal_slice\n","\n","roi_size = (160, 160, 32)\n","sw_batch_size = 64\n","slice_number = 95\n","\n","with torch.no_grad():\n","    for i, data in enumerate(test_loader):\n","\n","        predict = sliding_window_inference(data[\"image\"].to(device), roi_size,\n","                          sw_batch_size, model, progress=True, overlap=0.70)\n","        \n","        visualize_coronal_slice(data, predict, slice_number, f\"{bestmodel_filename}\\nepoch: {best_epoch}, best_metric: {best_metric}\", Norm=False)\n","        if i == 0:\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, RandSpatialCropSamplesd, CenterSpatialCropd\n","from monai.data import CacheDataset, DataLoader, Dataset\n","from monai.transforms import Transform\n","\n","from monai.transforms import MapTransform\n","\n","class ClampNegative(MapTransform):\n","    \"\"\"\n","    A MONAI transform that sets negative pixel values to zero within a dictionary format.\n","    This is useful for ensuring that the output predictions do not have negative values.\n","    Operates on all specified keys.\n","    \"\"\"\n","    def __init__(self, keys):\n","        super().__init__(keys)\n","    \n","    def __call__(self, data):\n","        for key in self.keys:\n","            d = data[key]\n","            print( d[d < 0])\n","            d[d < 0] = 0\n","            data[key] = d\n","        return data\n","\n","    \n","\n","roi_size = (160, 160, 32)\n","sw_batch_size = 64\n","\n","\n","post_transforms = Compose(\n","    [\n","        Invertd(\n","            keys=\"pred\",\n","            transform=loader_factory.get_test_transforms(),\n","            orig_keys=\"image\",\n","            meta_keys=\"pred_meta_dict\",\n","            orig_meta_keys=\"image_meta_dict\",\n","            meta_key_postfix=\"meta_dict\",\n","            nearest_interp=False,\n","            to_tensor=True,\n","        ),\n","        ClampNegative(keys=[\"pred\"]),\n","        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=output_dir, output_postfix=\"dl_dyn3\", resample=False), \n","    ]\n",")\n","\n","\n","with torch.no_grad():\n","    for test_data in test_loader:\n","        test_inputs = test_data[\"image\"].to(device)\n","        test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model, overlap=0.90)\n","        post_processed = [post_transforms(i) for i in decollate_batch(test_data)]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import os\n","# import glob\n","# from utils import PairFinder\n","# hint = 'dl_dyn2'\n","# hint = 'gamodel_3_18_onfdg'\n","# hint = 'Areset_test'\n","\n","# pair_finder = PairFinder(f'{fdg_data_dir}/MAC', output_dir, hint)\n","# test_dict_list = pair_finder.find_file_pairs()\n","# test_dict_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import nibabel as nib\n","# import os\n","\n","# for item in test_dict_list:\n","#     predicted_image_path = item['predicted']\n","#     reference_image_path = item['reference']\n","    \n","#     # Handle the '.nii.gz' extension properly\n","#     if predicted_image_path.endswith('.nii.gz'):\n","#         base_path = predicted_image_path[:-7]  # Remove '.nii.gz' from the end\n","#         corrected_dl_image_path = f\"{base_path}_corr.nii.gz\"\n","#     else:\n","#         # For other extensions, split and append '_corrected' before the extension\n","#         base_path, ext = os.path.splitext(predicted_image_path)\n","#         corrected_dl_image_path = f\"{base_path}_corr{ext}\"\n","    \n","#     # Load the reference (original) image to get its affine and header\n","#     orig = nib.load(reference_image_path)\n","    \n","#     # Load the DL predicted image\n","#     dl = nib.load(predicted_image_path)\n","#     dl_data = dl.get_fdata()\n","    \n","#     # Create a new NIfTI image using the DL data but with the original affine and header\n","#     corrected_dl_img = nib.Nifti1Image(dl_data, orig.affine, header=orig.header.copy())\n","    \n","#     # Save the corrected DL image\n","#     nib.save(corrected_dl_img, corrected_dl_image_path)\n"," \n","#     print(f\"Corrected image saved to: {corrected_dl_image_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Second method:\n","# import numpy as np\n","# import nibabel as nib\n","# import os\n","\n","# def write_nifti(data, file_name, affine, output_dtype=np.float32):\n","#     \"\"\"\n","#     Writes the given NIfTI data to a file.\n","\n","#     Parameters:\n","#     - data: The image data to write.\n","#     - file_name: The path to the file where the data should be saved.\n","#     - affine: The affine matrix for the NIfTI image.\n","#     - output_dtype: The desired data type for the saved image.\n","#     \"\"\"\n","#     img = nib.Nifti1Image(data.astype(output_dtype), affine)\n","#     img.to_filename(file_name)\n","\n","# # Assuming test_dict_list is defined as shown in your example\n","# for item in test_dict_list:\n","#     predicted_image_path = item['predicted']\n","#     reference_image_path = item['reference']\n","    \n","#     if predicted_image_path.endswith('.nii.gz'):\n","#         base_path = predicted_image_path[:-7]  # Correctly handles '.nii.gz' extension\n","#         corrected_dl_image_path = f\"{base_path}_corr2.nii.gz\"\n","#     else:\n","#         base_path, ext = os.path.splitext(predicted_image_path)\n","#         corrected_dl_image_path = f\"{base_path}_corr2{ext}\"\n","    \n","#     orig = nib.load(reference_image_path)\n","#     dl = nib.load(predicted_image_path)\n","#     dl_data = np.asanyarray(dl.dataobj)  # Directly accessing data\n","    \n","#     # Using write_nifti to save the corrected DL image\n","#     write_nifti(dl_data, corrected_dl_image_path, orig.affine, np.float32)\n","    \n","#     print(f\"Corrected image saved to: {corrected_dl_image_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import nibabel as nib\n","# import numpy as np\n","\n","# original_image_path = '/students/2023-2024/master/Shahpouri/DATA/FDG_TEST/MAC/006051_dataset_07_A.nii.gz'\n","# dl_image_path = '/students/2023-2024/master/Shahpouri/OUTPUT/006051_dataset_07_A/006051_dataset_07_A_test.nii.gz'\n","# corrected_dl_image_path = '/students/2023-2024/master/Shahpouri/OUTPUT/006051_dataset_07_A/006051_dataset_07_A_corrected.nii.gz'\n","\n","# # Load the original image to get its affine and header\n","# orig = nib.load(original_image_path)\n","\n","# # Load the DL predicted image\n","# dl = nib.load(dl_image_path)\n","# # Extract the data from the DL predicted image\n","# dl_data = dl.get_fdata()\n","\n","# # Create a new NIfTI image using the DL data but with the original affine and header\n","# corrected_dl_img = nib.Nifti1Image(dl_data, orig.affine, header=orig.header.copy())\n","\n","# # Save the DL image with the updated header to the corrected path\n","# nib.save(corrected_dl_img, corrected_dl_image_path)\n","\n","# # Load the newly saved corrected DL image\n","# dl_new = nib.load(corrected_dl_image_path)\n","\n","# # Print the affine matrix for verification\n","# print(\"Affine matrix for original image:\", orig.affine)\n","# print(\"Affine matrix for dl image:\", dl.affine)\n","# print(\"Affine matrix for corrected dl image:\", dl_new.affine)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1JG1lbZwQZ1TZS1EVyy_8ALByR6PaA5EJ","timestamp":1705418268395},{"file_id":"1E8ibC7ZVGwMZwubrqCgHcjS0GUHMG8-T","timestamp":1705308567486}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
