{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (EnsureChannelFirstd, Compose, CropForegroundd, LoadImaged, Orientationd, RandCropByPosNegLabeld, ScaleIntensityRanged, Spacingd)\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.apps import download_and_extract\n",
    "from monai.transforms import CenterSpatialCropd\n",
    "from monai.transforms import Resized\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from datetime import datetime\n",
    "from data_preparation import DataHandling \n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd,\n",
    "    SpatialPadd, ScaleIntensityd, CenterSpatialCropd\n",
    ")\n",
    "from monai.transforms import NormalizeIntensityd, RandWeightedCropd, RandSpatialCropSamplesd\n",
    "\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_file = 'config.json'\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "fdg_data_dir = config[\"fdg_data_dir\"]\n",
    "log_dir = config[\"log_dir\"]\n",
    "output_dir = config[\"output_dir\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 76/76 [01:27<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_handler = DataHandling(fdg_data_dir, train_mode=\"NAC\", target_mode=\"MAC\")\n",
    "\n",
    "train_files = data_handler.get_data_split('train')\n",
    "val_files = data_handler.get_data_split('val')\n",
    "test_files = data_handler.get_data_split('test')\n",
    "test_files\n",
    "\n",
    "from data_preparation import LoaderFactory\n",
    "loader_factory = LoaderFactory(\n",
    "    train_files=train_files,\n",
    "    val_files=val_files,\n",
    "    test_files=test_files,\n",
    "    patch_size = [168, 168, 16],\n",
    "    spacing = [4.07, 4.07, 3.00],\n",
    "    spatial_size = (168, 168, 320)\n",
    "    )\n",
    "\n",
    "# Get the DataLoader for each dataset type\n",
    "train_loader = loader_factory.get_loader('train', batch_size=4, num_workers=2, shuffle=True)\n",
    "val_loader = loader_factory.get_loader('val', batch_size=1, num_workers=2, shuffle=False)\n",
    "test_loader = loader_factory.get_loader('test', batch_size=1, num_workers=2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TrainingLogger:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.ensure_directory_exists(self.directory)\n",
    "        self.log_file = self.create_log_file()\n",
    "\n",
    "    def ensure_directory_exists(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    def create_log_file(self):\n",
    "        filename = f\"{self.directory}/log_{self.get_date()}.txt\"\n",
    "        return open(filename, \"w\")\n",
    "\n",
    "    def get_date(self):\n",
    "\n",
    "        s = datetime.now()\n",
    "        date = f\"{s.month}_{s.day}_{s.hour}_{s.minute}\"\n",
    "        return date\n",
    "\n",
    "    def log(self, message):\n",
    "        print(message)\n",
    "        self.log_file.write(message + \"\\n\")\n",
    "        self.log_file.flush()  # Flush the output buffer, ensuring immediate write to file\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        self.log_file.close()\n",
    "\n",
    "\n",
    "\n",
    "starting_epoch = 0\n",
    "decay_epoch = 4\n",
    "learning_rate = 0.00001\n",
    "\n",
    "import torch\n",
    "from monai.networks.nets import DynUNet\n",
    "from torch import nn\n",
    "\n",
    "class DecayLR:\n",
    "    def __init__(self, epochs, offset, decay_epochs):\n",
    "        epoch_flag = epochs - decay_epochs\n",
    "        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "        self.epochs = epochs\n",
    "        self.offset = offset\n",
    "        self.decay_epochs = decay_epochs\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (self.epochs - self.decay_epochs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DyUnet is set:\n",
      "Kernel size:  [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
      "Strides:  [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 1]]\n",
      "Defining optimizer...\n",
      "Defining scheduler...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from model_maker import get_network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_network(patch_size = [168, 168, 16], spacing = [4.07, 4.07, 3.00])\n",
    "model.load_state_dict(torch.load('/students/2023-2024/master/Shahpouri/LOG/model_3_18_22_10.pth'))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "print('Defining optimizer...')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "max_epochs = 20\n",
    "best_metric = float('inf')\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Define scheduler\n",
    "print('Defining scheduler...')\n",
    "lr_lambda = DecayLR(epochs=max_epochs, offset=0, decay_epochs=decay_epoch).step\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "\n",
    "def deep_loss(outputs, target, loss_function, device, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the deep supervision loss for each output feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: Tensor containing all output feature maps, including the final prediction.\n",
    "    - target: The ground truth tensor.\n",
    "    - loss_function: The loss function to apply.\n",
    "    - device: The device on which to perform the calculations.\n",
    "    - weights: A list of weights for each output's loss. Defaults to equal weighting if None.\n",
    "\n",
    "    Returns:\n",
    "    - Weighted average of the computed losses.\n",
    "    \"\"\"\n",
    "    # Unbind the outputs along the first dimension to handle each feature map individually\n",
    "    output_maps = torch.unbind(outputs, dim=1)\n",
    "    \n",
    "    if weights is None:\n",
    "        # If no weights specified, use equal weights\n",
    "        weights = [1.0 / len(output_maps)] * len(output_maps)\n",
    "    elif sum(weights) != 1:\n",
    "        # Normalize weights to sum to 1\n",
    "        total = sum(weights)\n",
    "        weights = [w / total for w in weights]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for output, weight in zip(output_maps, weights):\n",
    "        # Resize target to match the output size if necessary\n",
    "        resized_target = torch.nn.functional.interpolate(target, size=output.shape[2:], mode='nearest').to(device)\n",
    "        # Compute loss for the current output\n",
    "        loss = loss_function(output, resized_target)\n",
    "        # Accumulate weighted loss\n",
    "        total_loss += weight * loss\n",
    "\n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 76\n",
      "validation set: 20\n",
      "max_epochs: 20\n",
      "model.filters: [32, 64, 128, 256]\n",
      "----------\n",
      "epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/19, train_loss: 0.0598\n",
      "2/19, train_loss: 0.0467\n",
      "3/19, train_loss: 0.0441\n",
      "4/19, train_loss: 0.0381\n",
      "5/19, train_loss: 0.0299\n",
      "6/19, train_loss: 0.0278\n",
      "7/19, train_loss: 0.0237\n",
      "8/19, train_loss: 0.0233\n",
      "9/19, train_loss: 0.0218\n",
      "10/19, train_loss: 0.0244\n",
      "11/19, train_loss: 0.0156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(model, train_loader, val_loader, optimizer, loss_function, scheduler, max_epochs,log_dir, device)\n\u001b[1;32m    107\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog()\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# # L1 Regularization\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#         l1_reg = torch.tensor(0., requires_grad=True).to(device)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         for name, param in model.named_parameters():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m                 \u001b[38;5;66;03m# loss = self.loss_function(outputs, targets)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m                 \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m                 epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/torch/overrides.py:1621\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1616\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1617\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/torch/_tensor.py:1418\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1418\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader,\n",
    "                 optimizer, loss_function, scheduler, max_epochs,\n",
    "                 log_dir, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.scheduler = scheduler  # Add scheduler to the class initialization\n",
    "        self.max_epochs = max_epochs\n",
    "        self.val_interval = 2\n",
    "        self.directory = log_dir\n",
    "        self.device = device  # Assuming device is passed as a parameter\n",
    "        self.logger = TrainingLogger(log_dir)\n",
    "        self.best_metric = float('inf')\n",
    "        self.best_metric_epoch = -1\n",
    "\n",
    "\n",
    "    def log(self):\n",
    "        self.logger.log(f\"train set: {len(train_files)}\" )\n",
    "        self.logger.log(f\"validation set: {len(val_files)}\")\n",
    "        self.logger.log(f\"max_epochs: {max_epochs}\")\n",
    "        self.logger.log(f\"model.filters: {model.filters}\")\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.logger.log(\"-\" * 10)\n",
    "            self.logger.log(f\"epoch {epoch + 1}/{self.max_epochs}\")\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            step = 0\n",
    "\n",
    "            for batch_data in self.train_loader:\n",
    "                step += 1\n",
    "                inputs, targets = batch_data[\"image\"].to(self.device), batch_data[\"target\"].to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                # Check if deep supervision is used\n",
    "                if isinstance(outputs, tuple) or (outputs.dim() > targets.dim()):\n",
    "                    # Outputs from deep supervision\n",
    "                    loss = deep_loss(outputs, targets, loss_function, device)\n",
    "                else:\n",
    "                    # Standard output handling\n",
    "                    outputs = torch.squeeze(outputs)\n",
    "                    targets = torch.squeeze(targets, dim=1)  # Adjust for channel dimension if necessary\n",
    "                    loss = loss_function(outputs, targets)\n",
    "                \n",
    "# # L1 Regularization\n",
    "#         l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "#         for name, param in model.named_parameters():\n",
    "#             l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        \n",
    "#         loss += lambda_reg * l1_reg\n",
    "\n",
    "\n",
    "                # loss = self.loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                self.logger.log(f\"{step}/{len(self.train_loader.dataset) // self.train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "\n",
    "            epoch_loss /= step\n",
    "            self.logger.log(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            # Step the scheduler here, after the training phase and before the validation phase\n",
    "            self.scheduler.step()\n",
    "            self.logger.log(f\"current lr: {self.scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "            # Validation logic remains largely the same\n",
    "            if (epoch + 1) % self.val_interval == 0:\n",
    "                self.model.eval()\n",
    "                val_loss = 0\n",
    "                roi_size = (168, 168, 32)\n",
    "                sw_batch_size = 16\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for val_data in self.val_loader:\n",
    "                        val_inputs, val_targets = val_data[\"image\"].to(self.device), val_data[\"target\"].to(self.device)\n",
    "\n",
    "                        val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                        val_loss += self.loss_function(val_outputs, val_targets).item()\n",
    "\n",
    "                val_loss /= len(self.val_loader)\n",
    "                self.logger.log(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < self.best_metric:\n",
    "                    self.best_metric = val_loss\n",
    "                    self.best_metric_epoch = epoch + 1\n",
    "                    self.save_model()\n",
    "\n",
    "        self.logger.close()\n",
    "\n",
    "    def save_model(self):\n",
    "        model_filename = f\"model_{self.logger.get_date()}.pth\"\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.directory, model_filename))\n",
    "        self.logger.log(f\"Saved {model_filename} model, best_metric: {self.best_metric:.4f}, epoch: {self.best_metric_epoch}\")\n",
    "\n",
    "\n",
    "    \n",
    "trainer = ModelTrainer(model, train_loader, val_loader, optimizer, loss_function, scheduler, max_epochs,log_dir, device)\n",
    "trainer.log()\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
