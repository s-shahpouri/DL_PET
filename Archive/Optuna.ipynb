{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (EnsureChannelFirstd, Compose, CropForegroundd, LoadImaged, Orientationd, RandCropByPosNegLabeld, ScaleIntensityRanged, Spacingd)\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.apps import download_and_extract\n",
    "from monai.transforms import CenterSpatialCropd\n",
    "from monai.transforms import Resized\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from functools import partial\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, RandCropByPosNegLabeld, CenterSpatialCropd)\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/students/2023-2024/master/Shahpouri/DL_PET/config.json'\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "log_dir = config[\"log_dir\"]\n",
    "\n",
    "fdg_data_dir = config[\"fdg_data_dir\"]\n",
    "fdg_output_dir = config['fdg_output_dir']\n",
    "\n",
    "ga_output_dir = config[\"ga_output_dir\"]\n",
    "ga_data_dir = config[\"ga_data_dir\"]\n",
    "\n",
    "artifact_dir = config[\"artifacts\"]\n",
    "artifact_output_dir = config [\"artifact_output_dir\"]\n",
    "\n",
    "artifact_repeated_dir = config[\"artifact_repeated_dir\"]\n",
    "artifacts_repeated_output_dir = config[\"artifacts_repeated_output_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "33\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 129/129 [01:40<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import DataHandling \n",
    "\n",
    "data_handler = DataHandling(ga_data_dir, train_mode=\"NAC\", target_mode=\"ADCM\")\n",
    "\n",
    "train_files = data_handler.get_data_split('train')\n",
    "val_files = data_handler.get_data_split('val')\n",
    "test_files = data_handler.get_data_split('test')\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n",
    "\n",
    "from data_preparation import LoaderFactory\n",
    "loader_factory = LoaderFactory(\n",
    "    train_files=train_files,\n",
    "    val_files=val_files,\n",
    "    test_files=test_files,\n",
    "    num_samples = 20,\n",
    "    patch_size = [168, 168, 16],\n",
    "    spacing = [4.07, 4.07, 3.00],\n",
    "    # spacing = [1.92, 1.92, 3.27], # For fdg data\n",
    "    spatial_size = (168, 168, 320)\n",
    "    # spatial_size = (336, 336, 640) # for Fdg data\n",
    "    )\n",
    "\n",
    "train_loader = loader_factory.get_loader('train', batch_size=4, num_workers=2, shuffle=True)\n",
    "test_loader = loader_factory.get_loader('test', batch_size=1, num_workers=2, shuffle=False)\n",
    "val_loader = loader_factory.get_loader('val', batch_size=1, num_workers=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DyUnet is set:\n",
      "Kernel size:  [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
      "Strides:  [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model_maker import get_network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_network(patch_size = [168, 168, 16], spacing = [4.07, 4.07, 3.00])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Network Parameters Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "max_epochs = 2\n",
    "val_interval = 2\n",
    "best_metric = float('inf')\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_loss(outputs, target, loss_function, device, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the deep supervision loss for each output feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: Tensor containing all output feature maps, including the final prediction.\n",
    "    - target: The ground truth tensor.\n",
    "    - loss_function: The loss function to apply.\n",
    "    - device: The device on which to perform the calculations.\n",
    "    - weights: A list of weights for each output's loss. Defaults to equal weighting if None.\n",
    "\n",
    "    Returns:\n",
    "    - Weighted average of the computed losses.\n",
    "    \"\"\"\n",
    "    # Unbind the outputs along the first dimension to handle each feature map individually\n",
    "    output_maps = torch.unbind(outputs, dim=1)\n",
    "    \n",
    "    if weights is None:\n",
    "        # If no weights specified, use equal weights\n",
    "        weights = [1.0 / len(output_maps)] * len(output_maps)\n",
    "    elif sum(weights) != 1:\n",
    "        # Normalize weights to sum to 1\n",
    "        total = sum(weights)\n",
    "        weights = [w / total for w in weights]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for output, weight in zip(output_maps, weights):\n",
    "        # Resize target to match the output size if necessary\n",
    "        resized_target = torch.nn.functional.interpolate(target, size=output.shape[2:], mode='nearest').to(device)\n",
    "        # Compute loss for the current output\n",
    "        loss = loss_function(output, resized_target)\n",
    "        # Accumulate weighted loss\n",
    "        total_loss += weight * loss\n",
    "\n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "\n",
    "# def objective(trial, model, train_loader, val_loader, device, max_epochs, val_interval, loss_function, deep_loss):\n",
    "#     lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(max_epochs):\n",
    "\n",
    "#         epoch_loss = 0\n",
    "#         step = 0\n",
    "\n",
    "#         for batch_data in train_loader:\n",
    "#             step += 1\n",
    "#             inputs, targets = batch_data[\"image\"].to(device), batch_data[\"target\"].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             if isinstance(outputs, tuple) or (outputs.dim() > targets.dim()):\n",
    "#                 loss = deep_loss(outputs, targets, loss_function, device)\n",
    "#             else:\n",
    "#                 outputs = torch.squeeze(outputs)\n",
    "#                 targets = torch.squeeze(targets, dim=1)  # Adjust for channel dimension if necessary\n",
    "#                 loss = loss_function(outputs, targets)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         epoch_loss /= step\n",
    "\n",
    "\n",
    "#         # Validation logic remains largely the same\n",
    "#         if (epoch + 1) % val_interval == 0:\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             roi_size = (168, 168, 32)\n",
    "#             sw_batch_size = 16\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 for val_data in val_loader:\n",
    "#                     val_inputs, val_targets = val_data[\"image\"].to(device), val_data[\"target\"].to(device)\n",
    "\n",
    "#                     val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "#                     val_loss += loss_function(val_outputs, val_targets).item()\n",
    "\n",
    "#             val_loss /= len(val_loader)\n",
    "#     return val_loss\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Use functools.partial to pass extra arguments to the objective function\n",
    "# objective_with_args = functools.partial(\n",
    "#     objective,\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     device=device,\n",
    "#     max_epochs=max_epochs,\n",
    "#     val_interval=val_interval,\n",
    "#     loss_function=loss_function,\n",
    "#     deep_loss=deep_loss\n",
    "# )\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective_with_args, n_trials=10) \n",
    "\n",
    "# print(\"Best trial:\", study.best_trial.number)\n",
    "# print(\"Best value (validation loss):\", study.best_value)\n",
    "# print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "# Result for ADCM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2024-03-13 07:50:52,836] A new study created in memory with name: no-name-9bbcd5b9-2984-4e90-bb09-d75679945a16\n",
    "[I 2024-03-13 07:58:07,865] Trial 0 finished with value: 0.026119201553656775 and parameters: {'lr': 0.008072853294952104}. Best is trial 0 with value: 0.026119201553656775.\n",
    "[I 2024-03-13 08:05:24,497] Trial 1 finished with value: 0.024910740006495926 and parameters: {'lr': 0.05699468256166564}. Best is trial 1 with value: 0.024910740006495926.\n",
    "[I 2024-03-13 08:12:36,228] Trial 2 finished with value: 0.013740474365048987 and parameters: {'lr': 0.0009126015047894743}. Best is trial 2 with value: 0.013740474365048987.\n",
    "[I 2024-03-13 08:20:06,077] Trial 3 finished with value: 0.013423842097194317 and parameters: {'lr': 6.830399550473302e-05}. Best is trial 3 with value: 0.013423842097194317.\n",
    "[I 2024-03-13 08:27:29,051] Trial 4 finished with value: 0.012969923698726822 and parameters: {'lr': 0.00020615854432982388}. Best is trial 4 with value: 0.012969923698726822.\n",
    "[I 2024-03-13 08:35:12,140] Trial 5 finished with value: 0.012929829809924258 and parameters: {'lr': 3.557901372471996e-05}. Best is trial 5 with value: 0.012929829809924258.\n",
    "[I 2024-03-13 08:43:13,100] Trial 6 finished with value: 0.012899920029346557 and parameters: {'lr': 5.8365845148990886e-05}. Best is trial 6 with value: 0.012899920029346557.\n",
    "[I 2024-03-13 08:50:58,246] Trial 7 finished with value: 0.012893239296424915 and parameters: {'lr': 1.8475974753442264e-05}. Best is trial 7 with value: 0.012893239296424915.\n",
    "[I 2024-03-13 08:58:42,021] Trial 8 finished with value: 0.012873913805164835 and parameters: {'lr': 5.156637879231117e-05}. Best is trial 8 with value: 0.012873913805164835.\n",
    "[I 2024-03-13 09:06:27,394] Trial 9 finished with value: 0.012866821701583616 and parameters: {'lr': 1.82173317485587e-05}. Best is trial 9 with value: 0.012866821701583616.\n",
    "Best trial: 9\n",
    "Best value (validation loss): 0.012866821701583616\n",
    "Best hyperparameters: {'lr': 1.82173317485587e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-10 20:18:18,473] A new study created in memory with name: no-name-d99ec2da-cbb7-4b51-845a-90a5bb864b1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DyUnet is set:\n",
      "Kernel size:  [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]\n",
      "Strides:  [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   5%|▍         | 6/129 [00:04<01:35,  1.29it/s]\n",
      "[W 2024-05-10 20:18:23,222] Trial 0 failed with parameters: {'batch_size': 4, 'sw_batch_size': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 856, in next\n",
      "    item = self._items.popleft()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/zshahpouri/DLP/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3160577/866976852.py\", line 68, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, data_handler), n_trials=10)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3160577/866976852.py\", line 24, in objective\n",
      "    train_loader = loader_factory.get_loader('train', batch_size=batch_size, num_workers=2, shuffle=True)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/students/2023-2024/master/Shahpouri/DL_PET/data_preparation.py\", line 202, in get_loader\n",
      "    ds = Dataset(data=data_files, transform=transform) if dataset_type != \"train\" else CacheDataset(data=data_files, transform=transform, cache_rate=1.0, num_workers=num_workers)\n",
      "                                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/homes/zshahpouri/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py\", line 823, in __init__\n",
      "    self.set_data(data)\n",
      "  File \"/homes/zshahpouri/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py\", line 850, in set_data\n",
      "    self._cache = self._fill_cache(indices)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/homes/zshahpouri/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py\", line 879, in _fill_cache\n",
      "    return list(tqdm(p.imap(self._load_cache_item, indices), total=len(indices), desc=\"Loading dataset\"))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/homes/zshahpouri/DLP/.venv/lib/python3.11/site-packages/tqdm/std.py\", line 1182, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 861, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "[W 2024-05-10 20:18:23,236] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Optuna study\u001b[39;00m\n\u001b[1;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mnumber)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest value (validation loss):\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Optuna study\u001b[39;00m\n\u001b[1;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mnumber)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest value (validation loss):\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, data_handler)\u001b[0m\n\u001b[1;32m     13\u001b[0m sw_batch_size \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msw_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m32\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     15\u001b[0m loader_factory \u001b[38;5;241m=\u001b[39m LoaderFactory(\n\u001b[1;32m     16\u001b[0m     train_files\u001b[38;5;241m=\u001b[39mdata_handler\u001b[38;5;241m.\u001b[39mget_data_split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     17\u001b[0m     val_files\u001b[38;5;241m=\u001b[39mdata_handler\u001b[38;5;241m.\u001b[39mget_data_split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     spatial_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m168\u001b[39m, \u001b[38;5;241m168\u001b[39m, \u001b[38;5;241m320\u001b[39m)\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mloader_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m loader_factory\u001b[38;5;241m.\u001b[39mget_loader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m lr \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/students/2023-2024/master/Shahpouri/DL_PET/data_preparation.py:202\u001b[0m, in \u001b[0;36mLoaderFactory.get_loader\u001b[0;34m(self, dataset_type, batch_size, num_workers, shuffle)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files provided or unknown dataset type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     ds \u001b[38;5;241m=\u001b[39m Dataset(data\u001b[38;5;241m=\u001b[39mdata_files, transform\u001b[38;5;241m=\u001b[39mtransform) \u001b[38;5;28;01mif\u001b[39;00m dataset_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mCacheDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(ds, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py:823\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[0;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m ListProxy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py:850\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    847\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/monai/data/dataset.py:879\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[0;32m--> 879\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
      "File \u001b[0;32m~/DLP/.venv/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import optuna\n",
    "from model_maker import get_network\n",
    "from data_preparation import DataHandling, LoaderFactory\n",
    "\n",
    "def objective(trial, data_handler):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_network(patch_size=[168, 168, 16], spacing=[4.07, 4.07, 3.00])\n",
    "    model.to(device)\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 2, 16, step=2)\n",
    "    sw_batch_size = trial.suggest_int('sw_batch_size', 4, 32, step=4)\n",
    "    \n",
    "    loader_factory = LoaderFactory(\n",
    "        train_files=data_handler.get_data_split('train'),\n",
    "        val_files=data_handler.get_data_split('val'),\n",
    "        test_files=data_handler.get_data_split('test'),\n",
    "        patch_size=[168, 168, 16],\n",
    "        spacing=[4.07, 4.07, 3.00],\n",
    "        spatial_size=(168, 168, 320)\n",
    "    )\n",
    "    \n",
    "    train_loader = loader_factory.get_loader('train', batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "    val_loader = loader_factory.get_loader('val', batch_size=1, num_workers=2, shuffle=False)\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    max_epochs = 2\n",
    "    val_interval = 2\n",
    "\n",
    "    # Training and validation logic here\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_data in train_loader:\n",
    "            inputs, targets = batch_data[\"image\"].to(device), batch_data[\"target\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochloss += loss.item()\n",
    "\n",
    "        # Validation logic\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_targets = val_data[\"image\"].to(device), val_data[\"target\"].to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss += loss_function(val_outputs, val_targets).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Loading config and setting up data handling\n",
    "config_file = '/students/2023-2024/master/Shahpouri/DL_PET/config.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "data_handler = DataHandling(config[\"ga_data_dir\"], train_mode=\"NAC\", target_mode=\"ADCM\")\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, data_handler), n_trials=10)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (validation loss):\", study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
