{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (EnsureChannelFirstd, Compose, CropForegroundd, LoadImaged, Orientationd, RandCropByPosNegLabeld, ScaleIntensityRanged, Spacingd)\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.apps import download_and_extract\n",
    "from monai.transforms import CenterSpatialCropd\n",
    "from monai.transforms import Resized\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from datetime import datetime\n",
    "from data_preparation2 import DataHandling \n",
    "from UNet_model import create_unet\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from functools import partial\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, RandCropByPosNegLabeld, CenterSpatialCropd)\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/homes/zshahpouri/DLP/ASC-PET-001'\n",
    "directory = '/homes/zshahpouri/DLP/Practic/LOGTEST'\n",
    "output_dir = '/homes/zshahpouri/DLP/Practic/OUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images = sorted(glob.glob(os.path.join(data_dir, \"NAC\", \"*.nii.gz\")))\n",
    "target_images = sorted(glob.glob(os.path.join(data_dir, \"ADCM\", \"*.nii.gz\")))\n",
    "\n",
    "# data_dicts = [{\"image\": img, \"target\": tar} for img in train_images]\n",
    "data_dicts = [{\"image\": img, \"target\": tar} for img, tar in zip(train_images, target_images)]\n",
    "\n",
    "random.seed(42)\n",
    "# Separate data based on the center\n",
    "data_by_center = defaultdict(list)\n",
    "for data in data_dicts:\n",
    "    center = data[\"image\"].split('/')[-1].split('_')[1]  # Assuming the format is always like /path/Cx_...\n",
    "    # print(center)\n",
    "    data_by_center[center].append(data)\n",
    "# print(len(data_by_center['C5']))\n",
    "# Initialize test set with all data from C5\n",
    "test_files = data_by_center.pop('C5', [])\n",
    "\n",
    "# From each remaining center, randomly select 2 for the test set and ensure they're removed from the training set\n",
    "for center, files in data_by_center.items():\n",
    "    if len(files) > 2:  # Ensure there are more than 2 files to choose from\n",
    "        selected_for_test = random.sample(files, 2)\n",
    "        test_files.extend(selected_for_test)\n",
    "        # Remove selected files from the original list\n",
    "        for selected in selected_for_test:\n",
    "            files.remove(selected)\n",
    "    else:\n",
    "        test_files.extend(files)\n",
    "        data_by_center[center] = []  # Empty the list as all files have been moved to test\n",
    "\n",
    "# Recombine the remaining files for training and validation\n",
    "remaining_files = [file for files in data_by_center.values() for file in files]\n",
    "# print(len(remaining_files))\n",
    "random.shuffle(remaining_files)  # Shuffle to ensure random distribution\n",
    "\n",
    "total_size = len(remaining_files)\n",
    "train_size = math.floor(total_size * 0.8)\n",
    "\n",
    "train_files = remaining_files[:train_size]\n",
    "val_files = remaining_files[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import NormalizeIntensityd\n",
    "\n",
    "\n",
    "patch_size = [168, 168, 16]\n",
    "spacing = [4.07, 4.07, 3.00]\n",
    "spatial_size = (168, 168, 320)\n",
    "train_transforms = Compose(\n",
    "\n",
    "    [   LoadImaged(keys=[\"image\", \"target\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"target\"]),\n",
    "        # NormalizeIntensityd(keys=[ \"target\"]),\n",
    "        Spacingd(keys=[\"image\", \"target\"], pixdim= spacing, mode= 'trilinear'),\n",
    "        \n",
    "        SpatialPadd(keys=[\"image\", \"target\"], spatial_size=spatial_size, mode='constant'),  # Pad to ensure minimum size\n",
    "        \n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"target\"],\n",
    "            label_key=\"target\",\n",
    "            spatial_size = patch_size,\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),        ])\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [   LoadImaged(keys=[\"image\", \"target\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"target\"]),\n",
    "        Spacingd(keys=[\"image\", \"target\"], pixdim=spacing, mode= 'trilinear'),\n",
    "        SpatialPadd(keys=[\"image\", \"target\"], spatial_size=spatial_size, mode='constant'),  # Ensure minimum size\n",
    "        CenterSpatialCropd(keys=[\"image\", \"target\"], roi_size=spatial_size),  # Ensure uniform size\n",
    "    ])\n",
    "\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting_epoch = 0\n",
    "# decay_epoch = 5\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# import torch\n",
    "# from monai.networks.nets import DynUNet\n",
    "# from torch import nn\n",
    "\n",
    "# class DecayLR:\n",
    "#     def __init__(self, epochs, offset, decay_epochs):\n",
    "#         epoch_flag = epochs - decay_epochs\n",
    "#         assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n",
    "#         self.epochs = epochs\n",
    "#         self.offset = offset\n",
    "#         self.decay_epochs = decay_epochs\n",
    "\n",
    "#     def step(self, epoch):\n",
    "#         return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (self.epochs - self.decay_epochs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Network Parameters Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from monai.networks.nets import DynUNet\n",
    "\n",
    "def get_kernels_strides(patch_size, spacing):\n",
    "    \"\"\"\n",
    "    Adjusted function to use the correct variable names.\n",
    "    \"\"\"\n",
    "    sizes = patch_size  \n",
    "    spacings = spacing  \n",
    "    strides, kernels = [], []\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [2 if ratio <= 2 and size >= 8 else 1 for (ratio, size) in zip(spacing_ratio, sizes)]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
    "            if i % j != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Patch size is not supported, please try to modify the size {patch_size[idx]} in the spatial dimension {idx}.\"\n",
    "                )\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "\n",
    "\n",
    "def get_network(patch_size, spacing):\n",
    "    \"\"\"\n",
    "    Initializes the DynUNet with dynamically determined kernels and strides.\n",
    "    \"\"\"\n",
    "    kernels, strides = get_kernels_strides(patch_size, spacing)\n",
    "    print(kernels)\n",
    "    print(strides)\n",
    "    print(len(strides))\n",
    "    net = DynUNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        kernel_size=kernels,\n",
    "        strides=strides,\n",
    "        upsample_kernel_size=strides[1:],\n",
    "        norm_name=\"instance\",\n",
    "        deep_supervision=True,\n",
    "        deep_supr_num=2,\n",
    "    )\n",
    "    return net\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_network(patch_size, spacing)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "max_epochs = 2\n",
    "val_interval = 2\n",
    "best_metric = float('inf')\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_loss(outputs, target, loss_function, device, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the deep supervision loss for each output feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: Tensor containing all output feature maps, including the final prediction.\n",
    "    - target: The ground truth tensor.\n",
    "    - loss_function: The loss function to apply.\n",
    "    - device: The device on which to perform the calculations.\n",
    "    - weights: A list of weights for each output's loss. Defaults to equal weighting if None.\n",
    "\n",
    "    Returns:\n",
    "    - Weighted average of the computed losses.\n",
    "    \"\"\"\n",
    "    # Unbind the outputs along the first dimension to handle each feature map individually\n",
    "    output_maps = torch.unbind(outputs, dim=1)\n",
    "    \n",
    "    if weights is None:\n",
    "        # If no weights specified, use equal weights\n",
    "        weights = [1.0 / len(output_maps)] * len(output_maps)\n",
    "    elif sum(weights) != 1:\n",
    "        # Normalize weights to sum to 1\n",
    "        total = sum(weights)\n",
    "        weights = [w / total for w in weights]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for output, weight in zip(output_maps, weights):\n",
    "        # Resize target to match the output size if necessary\n",
    "        resized_target = torch.nn.functional.interpolate(target, size=output.shape[2:], mode='nearest').to(device)\n",
    "        # Compute loss for the current output\n",
    "        loss = loss_function(output, resized_target)\n",
    "        # Accumulate weighted loss\n",
    "        total_loss += weight * loss\n",
    "\n",
    "    return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def objective(trial, model, train_loader, val_loader, device, max_epochs, val_interval, loss_function, deep_loss):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, targets = batch_data[\"image\"].to(device), batch_data[\"target\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple) or (outputs.dim() > targets.dim()):\n",
    "                loss = deep_loss(outputs, targets, loss_function, device)\n",
    "            else:\n",
    "                outputs = torch.squeeze(outputs)\n",
    "                targets = torch.squeeze(targets, dim=1)  # Adjust for channel dimension if necessary\n",
    "                loss = loss_function(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= step\n",
    "\n",
    "\n",
    "        # Validation logic remains largely the same\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            roi_size = (168, 168, 32)\n",
    "            sw_batch_size = 16\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_targets = val_data[\"image\"].to(device), val_data[\"target\"].to(device)\n",
    "\n",
    "                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                    val_loss += loss_function(val_outputs, val_targets).item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use functools.partial to pass extra arguments to the objective function\n",
    "objective_with_args = functools.partial(\n",
    "    objective,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    max_epochs=max_epochs,\n",
    "    val_interval=val_interval,\n",
    "    loss_function=loss_function,\n",
    "    deep_loss=deep_loss\n",
    ")\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_with_args, n_trials=10) \n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (validation loss):\", study.best_value)\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "# Result for ADCM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2024-03-13 07:50:52,836] A new study created in memory with name: no-name-9bbcd5b9-2984-4e90-bb09-d75679945a16\n",
    "[I 2024-03-13 07:58:07,865] Trial 0 finished with value: 0.026119201553656775 and parameters: {'lr': 0.008072853294952104}. Best is trial 0 with value: 0.026119201553656775.\n",
    "[I 2024-03-13 08:05:24,497] Trial 1 finished with value: 0.024910740006495926 and parameters: {'lr': 0.05699468256166564}. Best is trial 1 with value: 0.024910740006495926.\n",
    "[I 2024-03-13 08:12:36,228] Trial 2 finished with value: 0.013740474365048987 and parameters: {'lr': 0.0009126015047894743}. Best is trial 2 with value: 0.013740474365048987.\n",
    "[I 2024-03-13 08:20:06,077] Trial 3 finished with value: 0.013423842097194317 and parameters: {'lr': 6.830399550473302e-05}. Best is trial 3 with value: 0.013423842097194317.\n",
    "[I 2024-03-13 08:27:29,051] Trial 4 finished with value: 0.012969923698726822 and parameters: {'lr': 0.00020615854432982388}. Best is trial 4 with value: 0.012969923698726822.\n",
    "[I 2024-03-13 08:35:12,140] Trial 5 finished with value: 0.012929829809924258 and parameters: {'lr': 3.557901372471996e-05}. Best is trial 5 with value: 0.012929829809924258.\n",
    "[I 2024-03-13 08:43:13,100] Trial 6 finished with value: 0.012899920029346557 and parameters: {'lr': 5.8365845148990886e-05}. Best is trial 6 with value: 0.012899920029346557.\n",
    "[I 2024-03-13 08:50:58,246] Trial 7 finished with value: 0.012893239296424915 and parameters: {'lr': 1.8475974753442264e-05}. Best is trial 7 with value: 0.012893239296424915.\n",
    "[I 2024-03-13 08:58:42,021] Trial 8 finished with value: 0.012873913805164835 and parameters: {'lr': 5.156637879231117e-05}. Best is trial 8 with value: 0.012873913805164835.\n",
    "[I 2024-03-13 09:06:27,394] Trial 9 finished with value: 0.012866821701583616 and parameters: {'lr': 1.82173317485587e-05}. Best is trial 9 with value: 0.012866821701583616.\n",
    "Best trial: 9\n",
    "Best value (validation loss): 0.012866821701583616\n",
    "Best hyperparameters: {'lr': 1.82173317485587e-05}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
